{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from http://www.mikaelbrunila.fi/2017/03/27/scraping-extracting-mapping-geodata-twitter/\n",
    "# https://marcobonzanini.com/2015/03/02/mining-twitter-data-with-python-part-1/\n",
    "\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "import re\n",
    "import collections\n",
    "import json_lines\n",
    "import csv\n",
    "\n",
    "consumer_key = '93WP2pKvICGqCvrdA6FWMbFLB'\n",
    "consumer_secret = 'LWcPKp3oqEmYTTbVaNpRD5tgqLl2KAfe2bNGsBk5xwTjxLxqHH'\n",
    "access_token = '3080541417-QQ15kHSfkZYBWWSqLHwKwfqB3INeYgxpY3P9IbO'\n",
    "access_secret = '2nuMVxvX8TzMyl1xHkUvElNy6BqWjqdoolTvLLxwoXHi1'\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = api.geo_search(query=\"USA\", granularity=\"country\")\n",
    "place_id = places[0].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = api.search(q=\"place:%s\" % place_id)\n",
    "for tweet in tweets:\n",
    "    print(tweet.text + \" | \" + tweet.place.name if tweet.place else \"Undefined place\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barbieri_clean_up(tweet,original = True):\n",
    "    #casefold() lowercases all words. Replace corrects for emojis that are not separated by space to words.\n",
    "    if original:\n",
    "        raw = tweet.text.casefold().encode(encoding = 'unicode-escape').decode().replace('\\\\U',' \\\\U').split()\n",
    "    else:\n",
    "        raw = tweet.casefold().encode(encoding = 'unicode-escape').decode().replace('\\\\U',' \\\\U').split()\n",
    "    #remove all hyperlinks\n",
    "    new_raw = []\n",
    "    for item in raw:\n",
    "        if item.find('http') == -1:\n",
    "            if item.find('#') == -1:\n",
    "                if item.find('@') == -1:\n",
    "                    new_raw.append(item)\n",
    "    return ' '.join(new_raw)\n",
    "\n",
    "def emoji_count(clean_tweet):\n",
    "    i = 0\n",
    "    for item in clean_tweet:\n",
    "        if item.find('\\\\U') > -1:\n",
    "            i += 1\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(tweet.text) for tweet in tweets]\n",
    "print()\n",
    "for tweet in tweets:\n",
    "    clean_tweet = barbieri_clean_up(tweet)\n",
    "    print(clean_tweet)\n",
    "    print()\n",
    "    print(emoji_count(clean_tweet))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './data/tweet_id_files/twitter-events-2012-2016/'\n",
    "paths = [folder + '2012-euro2012.jsonl',folder + '2012-mexican-election.jsonl',folder + '2012-superbowl.jsonl',\n",
    "         folder + '2012-sxsw.jsonl']\n",
    "regex = re.compile(r'\\\\d+(.*?)(?:\\\\u263a|\\\\U0001f645)')\n",
    "counts = collections.Counter()\n",
    "filtered_tweets_text = []\n",
    "with open(\"output_1.csv\", \"w\") as csv:\n",
    "    for path in paths:\n",
    "        print(path)\n",
    "        f = open(path,'rb')\n",
    "        for line in json_lines.reader(f):\n",
    "            tweet = line\n",
    "            if 'metadata' in tweet:\n",
    "                l = tweet.metadata['iso_language_code']\n",
    "            else:\n",
    "                l = tweet['lang']\n",
    "            if l == 'en':\n",
    "                if 'full_text' in tweet:\n",
    "                    text = tweet['full_text']\n",
    "                else:\n",
    "                    text = tweet['text']\n",
    "                e = re.findall(u'[\\\\U0001f600-\\\\U0001f650]', text)\n",
    "                if len(e)>0:\n",
    "                    clean = barbieri_clean_up(text,original = False)\n",
    "                    filtered_tweets_text += [clean]\n",
    "                    row = clean + '\\n'\n",
    "                    csv.write(row)\n",
    "                    for char in e:\n",
    "                        counts[char] += 1\n",
    "        f.close()\n",
    "          \n",
    "    for char, count in counts.most_common():\n",
    "        print(\"%s %5i\" % (char, count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
