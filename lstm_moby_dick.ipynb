{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidhou8791/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import utils\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_vectors = pickle.load(open('data/emoji_vectors.p', 'rb'))\n",
    "moby_dick_vectors = pickle.load(open('data/moby_dick_vectors.p', 'rb'))\n",
    "moby_dick_sents = pickle.load(open('data/moby_dick_sents.p', 'rb'))\n",
    "\n",
    "emoji_embedding = np.array([v for v in emoji_vectors.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "# sess.close()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "nodes = 300\n",
    "embed_size = 300\n",
    "x_seq_length = 32\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, (None, x_seq_length, embed_size), 'inputs')\n",
    "input_mean = tf.nn.l2_normalize(tf.reduce_mean(inputs, axis=1), axis=1, name='input_mean')\n",
    "\n",
    "output_embedding = tf.constant(emoji_embedding, name='output_embedding')\n",
    "\n",
    "with tf.name_scope('network'):\n",
    "    lstm_cell = tf.contrib.rnn.LSTMCell(nodes, name='lstm')\n",
    "    lstm_outputs, _ = tf.nn.dynamic_rnn(lstm_cell, inputs=inputs, dtype=tf.float32)\n",
    "\n",
    "    logits = tf.layers.dense(lstm_outputs, units=len(emoji_vectors), activation='softmax', name='dense') \n",
    "    outputs = utils.matmul3d(logits, output_embedding)\n",
    "\n",
    "    output_mean = tf.nn.l2_normalize(tf.reduce_mean(outputs, axis=1), axis=1)\n",
    "\n",
    "with tf.name_scope(\"optimization\"):\n",
    "    loss = tf.losses.cosine_distance(input_mean, output_mean, axis=1)\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "    \n",
    "tf.summary.scalar('loss', loss)\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter('models/lstm_moby_dick', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents, test_sents = train_test_split(moby_dick_sents, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [[moby_dick_vectors[w] for w in s] for s in train_sents]\n",
    "X_test = [[moby_dick_vectors[w] for w in s] for s in test_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7544 2515\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, batch_size):\n",
    "    i = 0\n",
    "    while i < len(X):\n",
    "        yield X[i:i+batch_size]\n",
    "        i += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('models/lstm_moby_dick/2', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Average Loss:  0.408 Epoch duration: 36.128s\n",
      "Epoch   1 Average Loss:  0.382 Epoch duration: 35.834s\n",
      "Epoch   2 Average Loss:  0.380 Epoch duration: 35.767s\n",
      "Epoch   3 Average Loss:  0.375 Epoch duration: 35.723s\n",
      "Epoch   4 Average Loss:  0.362 Epoch duration: 36.544s\n",
      "Epoch   5 Average Loss:  0.353 Epoch duration: 36.452s\n",
      "Epoch   6 Average Loss:  0.346 Epoch duration: 35.444s\n",
      "Epoch   7 Average Loss:  0.340 Epoch duration: 35.490s\n",
      "Epoch   8 Average Loss:  0.336 Epoch duration: 35.583s\n",
      "Epoch   9 Average Loss:  0.334 Epoch duration: 35.638s\n",
      "Epoch  10 Average Loss:  0.331 Epoch duration: 35.519s\n",
      "Epoch  11 Average Loss:  0.329 Epoch duration: 35.629s\n",
      "Epoch  12 Average Loss:  0.327 Epoch duration: 35.406s\n",
      "Epoch  13 Average Loss:  0.325 Epoch duration: 36.474s\n",
      "Epoch  14 Average Loss:  0.323 Epoch duration: 36.040s\n",
      "Epoch  15 Average Loss:  0.322 Epoch duration: 35.474s\n",
      "Epoch  16 Average Loss:  0.321 Epoch duration: 35.507s\n",
      "Epoch  17 Average Loss:  0.319 Epoch duration: 35.461s\n",
      "Epoch  18 Average Loss:  0.318 Epoch duration: 35.478s\n",
      "Epoch  19 Average Loss:  0.317 Epoch duration: 35.434s\n",
      "Epoch  20 Average Loss:  0.316 Epoch duration: 35.432s\n",
      "Epoch  21 Average Loss:  0.315 Epoch duration: 35.407s\n",
      "Epoch  22 Average Loss:  0.314 Epoch duration: 37.038s\n",
      "Epoch  23 Average Loss:  0.313 Epoch duration: 35.347s\n",
      "Epoch  24 Average Loss:  0.312 Epoch duration: 35.420s\n",
      "Epoch  25 Average Loss:  0.311 Epoch duration: 35.419s\n",
      "Epoch  26 Average Loss:  0.310 Epoch duration: 35.427s\n",
      "Epoch  27 Average Loss:  0.310 Epoch duration: 35.368s\n",
      "Epoch  28 Average Loss:  0.309 Epoch duration: 35.458s\n",
      "Epoch  29 Average Loss:  0.308 Epoch duration: 35.424s\n",
      "Epoch  30 Average Loss:  0.308 Epoch duration: 35.537s\n",
      "Epoch  31 Average Loss:  0.307 Epoch duration: 37.024s\n",
      "Epoch  32 Average Loss:  0.307 Epoch duration: 35.355s\n",
      "Epoch  33 Average Loss:  0.306 Epoch duration: 35.482s\n",
      "Epoch  34 Average Loss:  0.306 Epoch duration: 35.489s\n",
      "Epoch  35 Average Loss:  0.305 Epoch duration: 35.448s\n",
      "Epoch  36 Average Loss:  0.305 Epoch duration: 35.489s\n",
      "Epoch  37 Average Loss:  0.304 Epoch duration: 35.451s\n",
      "Epoch  38 Average Loss:  0.304 Epoch duration: 35.636s\n",
      "Epoch  39 Average Loss:  0.303 Epoch duration: 36.929s\n",
      "Epoch  40 Average Loss:  0.303 Epoch duration: 35.719s\n",
      "Epoch  41 Average Loss:  0.302 Epoch duration: 35.345s\n",
      "Epoch  42 Average Loss:  0.302 Epoch duration: 35.386s\n",
      "Epoch  43 Average Loss:  0.302 Epoch duration: 35.420s\n",
      "Epoch  44 Average Loss:  0.301 Epoch duration: 35.444s\n",
      "Epoch  45 Average Loss:  0.301 Epoch duration: 35.367s\n",
      "Epoch  46 Average Loss:  0.301 Epoch duration: 35.529s\n",
      "Epoch  47 Average Loss:  0.300 Epoch duration: 35.494s\n",
      "Epoch  48 Average Loss:  0.300 Epoch duration: 37.782s\n",
      "Epoch  49 Average Loss:  0.300 Epoch duration: 35.432s\n",
      "Epoch  50 Average Loss:  0.299 Epoch duration: 35.384s\n",
      "Epoch  51 Average Loss:  0.299 Epoch duration: 35.465s\n",
      "Epoch  52 Average Loss:  0.299 Epoch duration: 35.475s\n",
      "Epoch  53 Average Loss:  0.298 Epoch duration: 35.416s\n",
      "Epoch  54 Average Loss:  0.298 Epoch duration: 35.450s\n",
      "Epoch  55 Average Loss:  0.298 Epoch duration: 35.506s\n",
      "Epoch  56 Average Loss:  0.298 Epoch duration: 35.520s\n",
      "Epoch  57 Average Loss:  0.298 Epoch duration: 37.720s\n",
      "Epoch  58 Average Loss:  0.298 Epoch duration: 35.320s\n",
      "Epoch  59 Average Loss:  0.297 Epoch duration: 35.489s\n",
      "Epoch  60 Average Loss:  0.297 Epoch duration: 35.451s\n",
      "Epoch  61 Average Loss:  0.297 Epoch duration: 35.419s\n",
      "Epoch  62 Average Loss:  0.296 Epoch duration: 35.571s\n",
      "Epoch  63 Average Loss:  0.296 Epoch duration: 35.442s\n",
      "Epoch  64 Average Loss:  0.296 Epoch duration: 35.500s\n",
      "Epoch  65 Average Loss:  0.296 Epoch duration: 37.174s\n",
      "Epoch  66 Average Loss:  0.296 Epoch duration: 35.737s\n",
      "Epoch  67 Average Loss:  0.296 Epoch duration: 35.368s\n",
      "Epoch  68 Average Loss:  0.295 Epoch duration: 35.468s\n",
      "Epoch  69 Average Loss:  0.295 Epoch duration: 35.429s\n",
      "Epoch  70 Average Loss:  0.295 Epoch duration: 35.526s\n",
      "Epoch  71 Average Loss:  0.294 Epoch duration: 35.539s\n",
      "Epoch  72 Average Loss:  0.294 Epoch duration: 35.524s\n",
      "Epoch  73 Average Loss:  0.294 Epoch duration: 35.435s\n",
      "Epoch  74 Average Loss:  0.294 Epoch duration: 37.128s\n",
      "Epoch  75 Average Loss:  0.294 Epoch duration: 35.443s\n",
      "Epoch  76 Average Loss:  0.293 Epoch duration: 35.447s\n",
      "Epoch  77 Average Loss:  0.293 Epoch duration: 35.568s\n",
      "Epoch  78 Average Loss:  0.293 Epoch duration: 35.444s\n",
      "Epoch  79 Average Loss:  0.293 Epoch duration: 35.517s\n",
      "Epoch  80 Average Loss:  0.293 Epoch duration: 35.491s\n",
      "Epoch  81 Average Loss:  0.292 Epoch duration: 35.464s\n",
      "Epoch  82 Average Loss:  0.292 Epoch duration: 35.701s\n",
      "Epoch  83 Average Loss:  0.292 Epoch duration: 37.014s\n",
      "Epoch  84 Average Loss:  0.292 Epoch duration: 35.388s\n",
      "Epoch  85 Average Loss:  0.292 Epoch duration: 35.499s\n",
      "Epoch  86 Average Loss:  0.292 Epoch duration: 35.500s\n",
      "Epoch  87 Average Loss:  0.292 Epoch duration: 35.434s\n",
      "Epoch  88 Average Loss:  0.291 Epoch duration: 35.380s\n",
      "Epoch  89 Average Loss:  0.291 Epoch duration: 35.378s\n",
      "Epoch  90 Average Loss:  0.291 Epoch duration: 35.500s\n",
      "Epoch  91 Average Loss:  0.291 Epoch duration: 36.047s\n",
      "Epoch  92 Average Loss:  0.291 Epoch duration: 36.500s\n",
      "Epoch  93 Average Loss:  0.291 Epoch duration: 35.413s\n",
      "Epoch  94 Average Loss:  0.291 Epoch duration: 35.479s\n",
      "Epoch  95 Average Loss:  0.290 Epoch duration: 35.654s\n",
      "Epoch  96 Average Loss:  0.290 Epoch duration: 35.441s\n",
      "Epoch  97 Average Loss:  0.290 Epoch duration: 35.480s\n",
      "Epoch  98 Average Loss:  0.290 Epoch duration: 35.530s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2515 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  99 Average Loss:  0.290 Epoch duration: 35.400s\n",
      "Total training time: 3568.2733938694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2515/2515 [00:38<00:00, 65.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss: 0.30574346\n",
      "Test sentence: an old pike head sir there were seams dents in it\n",
      "Prediction: {'🈁', '🐬', '🤕', '🌮', '🈴', '🏰', '⁉'}\n",
      "Cosine distance: 0.21970135\n",
      "\n",
      "Test sentence: this one poor hunt then the best lance out all surely he will not hang back when every foremast hand has clutched whetstone\n",
      "Prediction: {'🈁', '🐬', '👏', '🐧', '🆙', '✳', '🏰', '⚜', '🤼', '🔂'}\n",
      "Cosine distance: 0.12055719\n",
      "\n",
      "Test sentence: drop them over fore aft\n",
      "Prediction: {'🦏', '🐬', '🆙', '🏰', '⚜'}\n",
      "Cosine distance: 0.35074228\n",
      "\n",
      "Test sentence: in the infancy the first settlement the emigrants were several times saved from starvation by the benevolent biscuit the whale ship luckily dropping an anchor in their waters\n",
      "Prediction: {'⚰', '⏮', '⛵', '〽', '🆘', '🐬', '🈁', '🕦', '🆙', '✳', '🏰', '♌'}\n",
      "Cosine distance: 0.17174953\n",
      "\n",
      "Test sentence: mighty whales which swim in sea water have sea oil swimming in them\n",
      "Prediction: {'🐠', '⛵', '🥘', '🐬', '⛽', '🈁', '🐧', '🏰', '🤼'}\n",
      "Cosine distance: 0.17046505\n",
      "\n",
      "Test sentence: round round the fish s back pinioned in the turns upon turns in which during the past night the whale had reeled the the lines around him the half torn body the\n",
      "Prediction: {'🐠', '🌃', '📏', '👁', '〽', '🐬', '🈁', '🆙', '✳', '🏰', '💔', '♨', '🕜', '❕'}\n",
      "Cosine distance: 0.14699328\n",
      "\n",
      "Test sentence: it is mild mild wind mild looking sky\n",
      "Prediction: {'🌡', '🈁', '🐬', '🐧', '🏰', '🌖'}\n",
      "Cosine distance: 0.3151219\n",
      "\n",
      "Test sentence: large thorough sweeping comprehension him it behooves me now unbutton him still further the points his hose unbuckling his garters casting loose the hooks the eyes the joints his innermost bones set\n",
      "Prediction: {'🉑', '👁', '🙏', '💪', '🈁', '🐬', '😰', '🌮', '🆙', '🈴', '5', '🖤', '🤷', '♨', '❕'}\n",
      "Cosine distance: 0.16269708\n",
      "\n",
      "Test sentence: could not fasten\n",
      "Prediction: {'🙏', '🏰', '🐬'}\n",
      "Cosine distance: 0.3631236\n",
      "\n",
      "Test sentence: you this\n",
      "Prediction: {'🏰', '🐬'}\n",
      "Cosine distance: 0.29817545\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "epochs = 100\n",
    "start = time.time()\n",
    "\n",
    "for i in range(epochs):\n",
    "    t = time.time()\n",
    "    losses = []\n",
    "    for X in utils.batch_generator(X_train, batch_size):\n",
    "        _, l, summary = sess.run([optimizer, loss, merged], feed_dict={inputs:X})\n",
    "        losses.append(l)\n",
    "    writer.add_summary(summary, global_step=i)\n",
    "#     if l < .0005:\n",
    "#         print('Epoch {:3} Loss: {:>6.3f} Epoch duration: {:>6.3f}s'.format(i, l, time.time() - t))\n",
    "#         break\n",
    "#     elif not i%10:\n",
    "    print('Epoch {:3} Average Loss: {:>6.3f} Epoch duration: {:>6.3f}s'.format(i, np.mean(losses, axis=-1), time.time() - t))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, 'models/lstm_moby_dick/model2')\n",
    "print('Total training time:', time.time()-start)\n",
    "\n",
    "predictions = []\n",
    "losses = []\n",
    "emoji_keys = list(emoji_vectors.keys())\n",
    "for x in tqdm(X_test):\n",
    "    lo, l = sess.run([logits, loss], feed_dict={inputs:np.array(x).reshape(-1, 32, 300)})\n",
    "    pred = np.argmax(lo, axis=2).reshape(32,)\n",
    "    predictions.append([emoji_keys[i] for i in pred])\n",
    "    losses.append(l)\n",
    "\n",
    "print('Average test loss:', np.mean(losses, axis=-1))\n",
    "\n",
    "for i in range(10):\n",
    "    print('Test sentence:', ' '.join(w for w in test_sents[i] if w))\n",
    "    print('Prediction:', set(predictions[i]))\n",
    "    print('Cosine distance:', losses[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/lstm_moby_dick/model2\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess, 'models/lstm_moby_dick/model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2515/2515 [00:29<00:00, 86.40it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "losses = []\n",
    "emoji_keys = list(emoji_vectors.keys())\n",
    "for x in tqdm(X_test):\n",
    "    lo, l = sess.run([logits, loss], feed_dict={inputs:np.array(x).reshape(-1, 32, 300)})\n",
    "    pred = np.argmax(lo, axis=2).reshape(32,)\n",
    "    predictions.append([emoji_keys[i] for i in pred])\n",
    "    losses.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = pickle.load(open('models/raw_train.p', 'rb'))\n",
    "raw_test = pickle.load(open('models/raw_test.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw: sentence: \" Welding an old pike - head , sir ; there were seams and dents in it .\"\n",
      "Test sentence: an old pike head sir there were seams dents in it\n",
      "Prediction: {'🏰', '🈁', '🐬', '🈴', '⁉', '🌮', '🤕'}\n",
      "Cosine distance: 0.21970135\n",
      "\n",
      "Raw: sentence: From this one poor hunt , then , the best lance out of all Nantucket , surely he will not hang back , when every foremast - hand has clutched a whetstone ?\n",
      "Test sentence: this one poor hunt then the best lance out all surely he will not hang back when every foremast hand has clutched whetstone\n",
      "Prediction: {'🆙', '🏰', '🈁', '🐬', '✳', '🔂', '⚜', '🤼', '🐧', '👏'}\n",
      "Cosine distance: 0.12055719\n",
      "\n",
      "Raw: sentence: drop them over , fore and aft .\n",
      "Test sentence: drop them over fore aft\n",
      "Prediction: {'🏰', '🆙', '🐬', '⚜', '🦏'}\n",
      "Cosine distance: 0.35074228\n",
      "\n",
      "Raw: sentence: Moreover , in the infancy of the first Australian settlement , the emigrants were several times saved from starvation by the benevolent biscuit of the whale - ship luckily dropping an anchor in their waters .\n",
      "Test sentence: in the infancy the first settlement the emigrants were several times saved from starvation by the benevolent biscuit the whale ship luckily dropping an anchor in their waters\n",
      "Prediction: {'🕦', '🏰', '🆙', '🈁', '🐬', '✳', '🆘', '⚰', '♌', '⏮', '⛵', '〽'}\n",
      "Cosine distance: 0.17174953\n",
      "\n",
      "Raw: sentence: \" The mighty whales which swim in a sea of water , and have a sea of oil swimming in them .\"\n",
      "Test sentence: mighty whales which swim in sea water have sea oil swimming in them\n",
      "Prediction: {'🏰', '🐠', '🈁', '🐬', '🤼', '⛽', '⛵', '🐧', '🥘'}\n",
      "Cosine distance: 0.17046505\n",
      "\n",
      "Raw: sentence: Lashed round and round to the fish ' s back ; pinioned in the turns upon turns in which , during the past night , the whale had reeled the involutions of the lines around him , the half torn body of the Parsee was seen ; his sable raiment frayed to shreds ; his distended eyes turned full upon old Ahab .\n",
      "Test sentence: round round the fish s back pinioned in the turns upon turns in which during the past night the whale had reeled the the lines around him the half torn body the\n",
      "Prediction: {'👁', '🐠', '🏰', '🆙', '🈁', '🌃', '🐬', '📏', '✳', '❕', '♨', '🕜', '〽', '💔'}\n",
      "Cosine distance: 0.14699328\n",
      "\n",
      "Raw: sentence: it is a mild , mild wind , and a mild looking sky .\n",
      "Test sentence: it is mild mild wind mild looking sky\n",
      "Prediction: {'🏰', '🈁', '🐬', '🌖', '🐧', '🌡'}\n",
      "Cosine distance: 0.3151219\n",
      "\n",
      "Raw: sentence: But to a large and thorough sweeping comprehension of him , it behooves me now to unbutton him still further , and untagging the points of his hose , unbuckling his garters , and casting loose the hooks and the eyes of the joints of his innermost bones , set him before you in his ultimatum ; that is to say , in his unconditional skeleton .\n",
      "Test sentence: large thorough sweeping comprehension him it behooves me now unbutton him still further the points his hose unbuckling his garters casting loose the hooks the eyes the joints his innermost bones set\n",
      "Prediction: {'😰', '👁', '💪', '🆙', '🖤', '🈁', '🐬', '🙏', '🉑', '❕', '🈴', '🌮', '🤷', '5', '♨'}\n",
      "Cosine distance: 0.16269708\n",
      "\n",
      "Raw: sentence: \" But could not fasten ?\"\n",
      "Test sentence: could not fasten\n",
      "Prediction: {'🐬', '🙏', '🏰'}\n",
      "Cosine distance: 0.3631236\n",
      "\n",
      "Raw: sentence: \" See you this ?\"\n",
      "Test sentence: you this\n",
      "Prediction: {'🐬', '🏰'}\n",
      "Cosine distance: 0.29817545\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('Raw: sentence:', ' '.join(w for w in raw_test[i] if w))\n",
    "    print('Test sentence:', ' '.join(w for w in test_sents[i] if w))\n",
    "    print('Prediction:', set(predictions[i]))\n",
    "    print('Cosine distance:', losses[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['have',\n",
       " 'striven',\n",
       " 'be',\n",
       " 'more',\n",
       " 'than',\n",
       " 'be',\n",
       " 'this',\n",
       " 'world',\n",
       " 's',\n",
       " 'or',\n",
       " 'mine',\n",
       " 'own',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sents[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
