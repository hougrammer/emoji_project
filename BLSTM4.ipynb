{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "===================\n",
      "Processing pretrained character embeds...\n",
      "Available characters for embedding\n",
      "number of characters in dictionary =  94\n",
      "number of dimensions =  300\n",
      "embedding_vectors.keys() =  dict_keys(['$', '(', ',', '0', '4', '8', '<', '@', 'D', 'H', 'L', 'P', 'T', 'X', '\\\\', '`', 'd', 'h', 'l', 'p', 't', 'x', '|', '#', \"'\", '+', '/', '3', '7', ';', '?', 'C', 'G', 'K', 'O', 'S', 'W', '[', '_', 'c', 'g', 'k', 'o', 's', 'w', '{', '\"', '&', '*', '.', '2', '6', ':', '>', 'B', 'F', 'J', 'N', 'R', 'V', 'Z', '^', 'b', 'f', 'j', 'n', 'r', 'v', 'z', '~', '!', '%', ')', '-', '1', '5', '9', '=', 'A', 'E', 'I', 'M', 'Q', 'U', 'Y', ']', 'a', 'e', 'i', 'm', 'q', 'u', 'y', '}'])\n",
      "\n",
      "===================\n",
      "===================\n",
      "Importing Barbieri dataset into list of list of characters\n",
      "size of entries_train =  293448\n",
      "size of entries_test =  4809\n",
      "size of entries_validation =  4718\n",
      "\n",
      "Preparing data for character embedding\n",
      "\n",
      "Converting dataset into character representation\n",
      "corpus length: 6483529\n",
      "total chars: 71\n",
      "\n",
      "Use indices_char[NUM] to lookup character by \n",
      "indices_char[3]  !\n",
      "indices_char[6]  $\n",
      "indices_char  {0: '\\t', 1: '\\n', 2: ' ', 3: '!', 4: '\"', 5: '#', 6: '$', 7: '%', 8: '&', 9: \"'\", 10: '(', 11: ')', 12: '*', 13: '+', 14: ',', 15: '-', 16: '.', 17: '/', 18: '0', 19: '1', 20: '2', 21: '3', 22: '4', 23: '5', 24: '6', 25: '7', 26: '8', 27: '9', 28: ':', 29: ';', 30: '<', 31: '=', 32: '>', 33: '?', 34: '@', 35: '[', 36: '\\\\', 37: ']', 38: '^', 39: '_', 40: '`', 41: 'a', 42: 'b', 43: 'c', 44: 'd', 45: 'e', 46: 'f', 47: 'g', 48: 'h', 49: 'i', 50: 'j', 51: 'k', 52: 'l', 53: 'm', 54: 'n', 55: 'o', 56: 'p', 57: 'q', 58: 'r', 59: 's', 60: 't', 61: 'u', 62: 'v', 63: 'w', 64: 'x', 65: 'y', 66: 'z', 67: '{', 68: '|', 69: '}', 70: '~'}\n",
      "\n",
      "char_indices  {'\\t': 0, '\\n': 1, ' ': 2, '!': 3, '\"': 4, '#': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, '*': 12, '+': 13, ',': 14, '-': 15, '.': 16, '/': 17, '0': 18, '1': 19, '2': 20, '3': 21, '4': 22, '5': 23, '6': 24, '7': 25, '8': 26, '9': 27, ':': 28, ';': 29, '<': 30, '=': 31, '>': 32, '?': 33, '@': 34, '[': 35, '\\\\': 36, ']': 37, '^': 38, '_': 39, '`': 40, 'a': 41, 'b': 42, 'c': 43, 'd': 44, 'e': 45, 'f': 46, 'g': 47, 'h': 48, 'i': 49, 'j': 50, 'k': 51, 'l': 52, 'm': 53, 'n': 54, 'o': 55, 'p': 56, 'q': 57, 'r': 58, 's': 59, 't': 60, 'u': 61, 'v': 62, 'w': 63, 'x': 64, 'y': 65, 'z': 66, '{': 67, '|': 68, '}': 69, '~': 70}\n",
      "\n",
      "Trimming tweets to MAX_SENT_LENGTH =  40\n",
      "sent_trim_train[0]  family @ town of ridgefield \n",
      "len(sent_trim_train[0])  28\n",
      "sent_trim_test[0]  funny how you change when certain people\n",
      "len(sent_trim_test[0])  40\n",
      "sent_trim_validation[0]  welcome to miami @ hilton miami airport \n",
      "len(sent_trim_validation[0])  40\n",
      "\n",
      "\n",
      "===================\n",
      "===================\n",
      "Vectorize the dataset (with padding)\n",
      "len(X_train[0]) =  40\n",
      "character embedding X_train[0] =  [46 41 53 49 52 65  2 34  2 60 55 63 54  2 55 46  2 58 49 44 47 45 46 49\n",
      " 45 52 44  2  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "len(X_test[0]) =  40\n",
      "character embedding X_test[0] =  [46 61 54 54 65  2 48 55 63  2 65 55 61  2 43 48 41 54 47 45  2 63 48 45\n",
      " 54  2 43 45 58 60 41 49 54  2 56 45 55 56 52 45]\n",
      "len(X_validation[0]) =  40\n",
      "character embedding X_validation[0] =  [63 45 52 43 55 53 45  2 60 55  2 53 49 41 53 49  2 34  2 48 49 52 60 55\n",
      " 54  2 53 49 41 53 49  2 41 49 58 56 55 58 60  2]\n",
      "DONE vectorizing sentences\n",
      "tweets stored in X_train\n",
      "\n",
      "\n",
      "===================\n",
      "===================\n",
      "Vectorizing emojis\n",
      "\n",
      "emoji_dict  {'eoji2764': 1, 'eoji1f602': 2, 'eoji1f60d': 3, 'eoji1f525': 4, 'eoji1f4af': 5}\n",
      "\n",
      "tweet emojis stored in emojis_embedded\n",
      "emojis_embedded_train[0:10] [1 4 3 4 3 1 3 1 2 1]\n",
      "emojis_embedded_test[0:10] [2 2 2 3 2 1 2 2 4 2]\n",
      "emojis_embedded_validation[0:10] [5 2 4 5 4 1 4 1 1 1]\n",
      "\n",
      "\n",
      "===================\n",
      "===================\n",
      "One hot encoding all vectors X and y\n",
      "selecting from dataset NUM_SAMPLES =  293448\n",
      "Use X_enc_train, X_enc_test, X_enc_validation, y_enc_train, y_enc_test, y_enc_validation\n",
      "DONE\n",
      "===================\n",
      "===================\n",
      "Checking data integrity of datasets\n",
      "\n",
      "Checking data integrity of X_enc_train\n",
      "Shape  (293448, 40, 71)\n",
      "Max value  1.0\n",
      "Min value  0.0\n",
      "Any NaN?  False\n",
      "Checking for zero arrays\n",
      "Checking for arrays with multiple ones\n",
      "\n",
      "Checking data integrity of X_enc_test\n",
      "Shape  (4809, 40, 71)\n",
      "Max value  1.0\n",
      "Min value  0.0\n",
      "Any NaN?  False\n",
      "Checking for zero arrays\n",
      "Checking for arrays with multiple ones\n",
      "\n",
      "Checking data integrity of X_enc_validation\n",
      "Shape  (4718, 40, 71)\n",
      "Max value  1.0\n",
      "Min value  0.0\n",
      "Any NaN?  False\n",
      "Checking for zero arrays\n",
      "Checking for arrays with multiple ones\n",
      "\n",
      "Checking data integrity of y_enc_train\n",
      "Shape  (293448, 5)\n",
      "Max value  1.0\n",
      "Min value  0.0\n",
      "Any NaN?  False\n",
      "Checking for zero arrays\n",
      "Checking for arrays with multiple ones\n",
      "\n",
      "Checking data integrity of y_enc_test\n",
      "Shape  (4809, 5)\n",
      "Max value  1.0\n",
      "Min value  0.0\n",
      "Any NaN?  False\n",
      "Checking for zero arrays\n",
      "Checking for arrays with multiple ones\n",
      "\n",
      "Checking data integrity of y_enc_validation\n",
      "Shape  (4718, 5)\n",
      "Max value  1.0\n",
      "Min value  0.0\n",
      "Any NaN?  False\n",
      "Checking for zero arrays\n",
      "Checking for arrays with multiple ones\n",
      "\n",
      "len(char_indices)  71\n",
      "len(indics_char)  71\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM, Dropout, CuDNNLSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional, LeakyReLU\n",
    "from keras import optimizers, backend, callbacks\n",
    "import keras_metrics\n",
    "\n",
    "%run -i 'create_char_embedded_dataset.py'\n",
    "\n",
    "y_enc_train = np.squeeze(y_enc_train,axis=1)\n",
    "y_enc_test = np.squeeze(y_enc_test,axis=1)\n",
    "y_enc_validation = np.squeeze(y_enc_validation,axis=1)\n",
    "\n",
    "\n",
    "# Test integrity of datasets\n",
    "\n",
    "print('===================')\n",
    "print('===================')\n",
    "print('Checking data integrity of datasets')\n",
    "print()\n",
    "\n",
    "def check_data_for_multiple_ones(dataset, sequence_size):\n",
    "    for i, sentence in enumerate(dataset):\n",
    "        if (np.count_nonzero(sentence) != sequence_size):\n",
    "            print('WARNING! multiple 1s found in array in dataset at index = ', i)\n",
    "            return\n",
    "\n",
    "def check_data_for_zero_array(dataset):\n",
    "    for sentence in dataset:\n",
    "        if np.any(sentence) is False:\n",
    "            print('WARNING! 0 array found in dataset')\n",
    "            return\n",
    "\n",
    "def check_data(dataset, dataset_name, sequence_size):\n",
    "    print('Checking data integrity of', dataset_name)\n",
    "    print('Shape ', dataset.shape)\n",
    "    print('Max value ', np.amax(dataset))\n",
    "    print('Min value ',np.amin(dataset))\n",
    "    print('Any NaN? ', np.isnan(np.min(dataset)))\n",
    "    print('Checking for zero arrays')\n",
    "    check_data_for_zero_array(dataset)\n",
    "    print('Checking for arrays with multiple ones')\n",
    "    check_data_for_multiple_ones(dataset, sequence_size)\n",
    "    print()\n",
    "\n",
    "check_data(X_enc_train, 'X_enc_train', 40)\n",
    "check_data(X_enc_test, 'X_enc_test', 40)\n",
    "check_data(X_enc_validation, 'X_enc_validation', 40)\n",
    "\n",
    "check_data(y_enc_train, 'y_enc_train', 1)\n",
    "check_data(y_enc_test, 'y_enc_test', 1)\n",
    "check_data(y_enc_validation, 'y_enc_validation', 1)\n",
    "\n",
    "print('len(char_indices) ',len(char_indices))\n",
    "print('len(indics_char) ',len(indices_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras backend using float type =  float32\n",
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# from keras.layers import CuDNNLSTM\n",
    "\n",
    "# backend.set_floatx('float64') # default is float 32 - recommended to fix loss = nan\n",
    "print('Keras backend using float type = ', backend.floatx())\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.reset_states() # Attempting to reinitialize model weights if running modules again\n",
    "\n",
    "# model.add(Dropout(0.4)) # Attempt to fix loss nan\n",
    "\n",
    "model.add( Bidirectional(\n",
    "#                         CuDNNLSTM(\n",
    "                        LSTM(\n",
    "                            128,\n",
    "#                             kernel_initializer='glorot_uniform', # Default is glorot_uniform\n",
    "#                              activation = 'relu', # default is tanh\n",
    "#                              inner_activation = 'relu', # default is hard_sigmoid\n",
    "#                              recurrent_activation = 'softmax',\n",
    "#                              dropout_W = 0.0, # default is 0.0\n",
    "#                              dropout_U = 0.0, # default is 0.0\n",
    "                             return_sequences=False,\n",
    "                             input_shape=(40,len(char_indices))\n",
    "                            )))\n",
    "\n",
    "\n",
    "# model.add(BatchNormalization()) # Attempt to fix loss nan\n",
    "# model.add(Dropout(0.4)) # Attempt to fix loss nan\n",
    "\n",
    "# Moved these to be before the dense so they are applied to output layers of LSTM\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(LeakyReLU(alpha=0.1)) # this was the biggest fix for the loss nan\n",
    "\n",
    "model.add( Dense(5,\n",
    "#                  activation='relu'  # Uses no activation by default\n",
    "                )\n",
    "         )\n",
    "\n",
    "# model.add(BatchNormalization()) # Attempt to fix loss nan\n",
    "# model.add(Dropout(0.4)) # Attempt to fix loss nan\n",
    "\n",
    "# If enabled here, these apply to the output of the dense 5D rather than LSTM 128D\n",
    "# model.add(Activation('relu'))\n",
    "model.add(LeakyReLU(alpha=0.1)) # this was the biggest fix for the loss nan\n",
    "\n",
    "\n",
    "\n",
    "optimiz = optimizers.Adam(\n",
    "#                         lr = 0.000001, # default lr = 0.001 - turned off because Adam dynamically adjusts\n",
    "#                         clipnorm = 1.0, # Suggested to help with NaN\n",
    "#                         clipvalue = 0.5\n",
    "                        )\n",
    "\n",
    "metrics = [\n",
    "              'accuracy',\n",
    "#               keras_metrics.precision(label=class_value),\n",
    "#               keras_metrics.recall(label=class_value)\n",
    "          ]\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = optimiz,\n",
    "              metrics = metrics\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 293448 samples, validate on 4718 samples\n",
      "Epoch 1/10\n",
      "293448/293448 [==============================] - 312s 1ms/step - loss: 1.4517 - acc: 0.4092 - val_loss: 1.3983 - val_acc: 0.4303\n",
      "Epoch 2/10\n",
      "293448/293448 [==============================] - 327s 1ms/step - loss: 1.3720 - acc: 0.4402 - val_loss: 1.3724 - val_acc: 0.4415\n",
      "Epoch 3/10\n",
      "293448/293448 [==============================] - 308s 1ms/step - loss: 1.3949 - acc: 0.4140 - val_loss: 1.3708 - val_acc: 0.4366\n",
      "Epoch 4/10\n",
      "293448/293448 [==============================] - 308s 1ms/step - loss: 1.3813 - acc: 0.4341 - val_loss: 1.4375 - val_acc: 0.3987\n",
      "Epoch 5/10\n",
      "293448/293448 [==============================] - 308s 1ms/step - loss: 1.3846 - acc: 0.4296 - val_loss: 1.4457 - val_acc: 0.4218\n",
      "Epoch 6/10\n",
      "293448/293448 [==============================] - 306s 1ms/step - loss: 1.3640 - acc: 0.4450 - val_loss: 1.3887 - val_acc: 0.4387\n",
      "Epoch 7/10\n",
      "293448/293448 [==============================] - 306s 1ms/step - loss: 1.3484 - acc: 0.4554 - val_loss: 1.3730 - val_acc: 0.4517\n",
      "Epoch 8/10\n",
      " 28160/293448 [=>............................] - ETA: 4:36 - loss: 1.3590 - acc: 0.4623"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "\n",
    "\n",
    "model.fit(X_enc_train, y_enc_train,\n",
    "          batch_size=512,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(X_enc_validation, y_enc_validation),\n",
    "          callbacks = [\n",
    "              callbacks.TerminateOnNaN() # Automatically terminate fit if loss = nan\n",
    "          ]\n",
    "         )\n",
    "\n",
    "# model.summary() # Use if model throws dimensions errors\n",
    "\n",
    "score, acc = model.evaluate(X_enc_test, y_enc_test)\n",
    "# score, acc, prec, rec = model.evaluate(X_enc_test, y_enc_test)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)\n",
    "# print('Test prec:', prec)\n",
    "# print('Test rec:', rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
