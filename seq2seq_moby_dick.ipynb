{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davidhou8791/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import utils\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_vectors = pickle.load(open('data/emoji_vectors.p', 'rb'))\n",
    "moby_dick_vectors = pickle.load(open('data/moby_dick_vectors.p', 'rb'))\n",
    "moby_dick_sents = pickle.load(open('data/moby_dick_sents.p', 'rb'))\n",
    "\n",
    "emoji_embedding = np.array([v for v in emoji_vectors.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess.close()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "nodes = 300\n",
    "embed_size = 300\n",
    "x_seq_length = 32\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, (None, x_seq_length, embed_size), 'inputs')\n",
    "input_mean = tf.nn.l2_normalize(tf.reduce_mean(inputs, axis=1), axis=1, name='input_mean')\n",
    "\n",
    "output_embedding = tf.constant(emoji_embedding, name='output_embedding')\n",
    "\n",
    "with tf.name_scope('network'):\n",
    "    lstm_encoder = tf.contrib.rnn.LSTMCell(nodes, name='lstm_encoder')\n",
    "    _, encoding = tf.nn.dynamic_rnn(lstm_encoder, inputs=inputs, dtype=tf.float32)\n",
    "    \n",
    "    lstm_decoder = tf.contrib.rnn.LSTMCell(nodes, name='lstm_decoder')\n",
    "    lstm_outputs, _ = tf.nn.dynamic_rnn(lstm_decoder, inputs=inputs, initial_state=encoding, dtype=tf.float32)\n",
    "\n",
    "    logits = tf.layers.dense(lstm_outputs, units=len(emoji_vectors), activation='softmax', name='dense') \n",
    "    outputs = utils.matmul3d(logits, output_embedding)\n",
    "\n",
    "    output_mean = tf.nn.l2_normalize(tf.reduce_mean(outputs, axis=1), axis=1)\n",
    "\n",
    "with tf.name_scope(\"optimization\"):\n",
    "    loss = tf.losses.cosine_distance(input_mean, output_mean, axis=1)\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "    \n",
    "tf.summary.scalar('loss', loss)\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter('models/seq2seq_moby_dick/1', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents, test_sents = train_test_split(moby_dick_sents, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [[moby_dick_vectors[w] for w in s] for s in train_sents]\n",
    "X_test = [[moby_dick_vectors[w] for w in s] for s in test_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, batch_size):\n",
    "    i = 0\n",
    "    while i < len(X):\n",
    "        yield X[i:i+batch_size]\n",
    "        i += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Average Loss:  0.406 Epoch duration: 48.220s\n",
      "Epoch   1 Average Loss:  0.380 Epoch duration: 47.975s\n",
      "Epoch   2 Average Loss:  0.379 Epoch duration: 47.631s\n",
      "Epoch   3 Average Loss:  0.371 Epoch duration: 47.624s\n",
      "Epoch   4 Average Loss:  0.358 Epoch duration: 48.705s\n",
      "Epoch   5 Average Loss:  0.350 Epoch duration: 47.596s\n",
      "Epoch   6 Average Loss:  0.343 Epoch duration: 48.086s\n",
      "Epoch   7 Average Loss:  0.339 Epoch duration: 47.785s\n",
      "Epoch   8 Average Loss:  0.335 Epoch duration: 47.947s\n",
      "Epoch   9 Average Loss:  0.332 Epoch duration: 47.948s\n",
      "Epoch  10 Average Loss:  0.330 Epoch duration: 47.780s\n",
      "Epoch  11 Average Loss:  0.328 Epoch duration: 48.719s\n",
      "Epoch  12 Average Loss:  0.325 Epoch duration: 48.195s\n",
      "Epoch  13 Average Loss:  0.322 Epoch duration: 47.696s\n",
      "Epoch  14 Average Loss:  0.320 Epoch duration: 47.751s\n",
      "Epoch  15 Average Loss:  0.318 Epoch duration: 47.605s\n",
      "Epoch  16 Average Loss:  0.317 Epoch duration: 47.565s\n",
      "Epoch  17 Average Loss:  0.315 Epoch duration: 48.736s\n",
      "Epoch  18 Average Loss:  0.314 Epoch duration: 48.156s\n",
      "Epoch  19 Average Loss:  0.313 Epoch duration: 47.711s\n",
      "Epoch  20 Average Loss:  0.312 Epoch duration: 47.491s\n",
      "Epoch  21 Average Loss:  0.311 Epoch duration: 48.008s\n",
      "Epoch  22 Average Loss:  0.310 Epoch duration: 48.338s\n",
      "Epoch  23 Average Loss:  0.309 Epoch duration: 48.318s\n",
      "Epoch  24 Average Loss:  0.309 Epoch duration: 48.835s\n",
      "Epoch  25 Average Loss:  0.308 Epoch duration: 48.549s\n",
      "Epoch  26 Average Loss:  0.307 Epoch duration: 48.014s\n",
      "Epoch  27 Average Loss:  0.307 Epoch duration: 48.308s\n",
      "Epoch  28 Average Loss:  0.306 Epoch duration: 48.223s\n",
      "Epoch  29 Average Loss:  0.306 Epoch duration: 48.160s\n",
      "Epoch  30 Average Loss:  0.306 Epoch duration: 49.344s\n",
      "Epoch  31 Average Loss:  0.305 Epoch duration: 48.799s\n",
      "Epoch  32 Average Loss:  0.305 Epoch duration: 48.191s\n",
      "Epoch  33 Average Loss:  0.305 Epoch duration: 48.072s\n",
      "Epoch  34 Average Loss:  0.304 Epoch duration: 48.109s\n",
      "Epoch  35 Average Loss:  0.304 Epoch duration: 48.162s\n",
      "Epoch  36 Average Loss:  0.304 Epoch duration: 48.995s\n",
      "Epoch  37 Average Loss:  0.303 Epoch duration: 48.628s\n",
      "Epoch  38 Average Loss:  0.303 Epoch duration: 48.126s\n",
      "Epoch  39 Average Loss:  0.302 Epoch duration: 48.090s\n",
      "Epoch  40 Average Loss:  0.302 Epoch duration: 47.862s\n",
      "Epoch  41 Average Loss:  0.302 Epoch duration: 48.027s\n",
      "Epoch  42 Average Loss:  0.302 Epoch duration: 48.006s\n",
      "Epoch  43 Average Loss:  0.301 Epoch duration: 49.269s\n",
      "Epoch  44 Average Loss:  0.301 Epoch duration: 48.449s\n",
      "Epoch  45 Average Loss:  0.301 Epoch duration: 47.839s\n",
      "Epoch  46 Average Loss:  0.300 Epoch duration: 47.782s\n",
      "Epoch  47 Average Loss:  0.300 Epoch duration: 47.733s\n",
      "Epoch  48 Average Loss:  0.300 Epoch duration: 47.891s\n",
      "Epoch  49 Average Loss:  0.300 Epoch duration: 48.890s\n",
      "Epoch  50 Average Loss:  0.299 Epoch duration: 48.428s\n",
      "Epoch  51 Average Loss:  0.299 Epoch duration: 47.492s\n",
      "Epoch  52 Average Loss:  0.299 Epoch duration: 47.536s\n",
      "Epoch  53 Average Loss:  0.299 Epoch duration: 47.562s\n",
      "Epoch  54 Average Loss:  0.299 Epoch duration: 47.583s\n",
      "Epoch  55 Average Loss:  0.299 Epoch duration: 47.760s\n",
      "Epoch  56 Average Loss:  0.299 Epoch duration: 48.977s\n",
      "Epoch  57 Average Loss:  0.298 Epoch duration: 48.023s\n",
      "Epoch  58 Average Loss:  0.298 Epoch duration: 47.744s\n",
      "Epoch  59 Average Loss:  0.298 Epoch duration: 47.841s\n",
      "Epoch  60 Average Loss:  0.298 Epoch duration: 47.342s\n",
      "Epoch  61 Average Loss:  0.298 Epoch duration: 47.532s\n",
      "Epoch  62 Average Loss:  0.297 Epoch duration: 48.994s\n",
      "Epoch  63 Average Loss:  0.297 Epoch duration: 48.250s\n",
      "Epoch  64 Average Loss:  0.297 Epoch duration: 47.490s\n",
      "Epoch  65 Average Loss:  0.297 Epoch duration: 47.429s\n",
      "Epoch  66 Average Loss:  0.297 Epoch duration: 47.649s\n",
      "Epoch  67 Average Loss:  0.297 Epoch duration: 47.629s\n",
      "Epoch  68 Average Loss:  0.297 Epoch duration: 47.646s\n",
      "Epoch  69 Average Loss:  0.297 Epoch duration: 49.279s\n",
      "Epoch  70 Average Loss:  0.296 Epoch duration: 47.701s\n",
      "Epoch  71 Average Loss:  0.296 Epoch duration: 47.359s\n",
      "Epoch  72 Average Loss:  0.296 Epoch duration: 47.400s\n",
      "Epoch  73 Average Loss:  0.296 Epoch duration: 47.588s\n",
      "Epoch  74 Average Loss:  0.296 Epoch duration: 47.466s\n",
      "Epoch  75 Average Loss:  0.295 Epoch duration: 48.526s\n",
      "Epoch  76 Average Loss:  0.295 Epoch duration: 48.158s\n",
      "Epoch  77 Average Loss:  0.295 Epoch duration: 47.610s\n",
      "Epoch  78 Average Loss:  0.295 Epoch duration: 47.676s\n",
      "Epoch  79 Average Loss:  0.295 Epoch duration: 47.688s\n",
      "Epoch  80 Average Loss:  0.295 Epoch duration: 47.678s\n",
      "Epoch  81 Average Loss:  0.295 Epoch duration: 47.619s\n",
      "Epoch  82 Average Loss:  0.295 Epoch duration: 49.281s\n",
      "Epoch  83 Average Loss:  0.294 Epoch duration: 47.930s\n",
      "Epoch  84 Average Loss:  0.294 Epoch duration: 47.522s\n",
      "Epoch  85 Average Loss:  0.294 Epoch duration: 47.513s\n",
      "Epoch  86 Average Loss:  0.294 Epoch duration: 47.619s\n",
      "Epoch  87 Average Loss:  0.294 Epoch duration: 47.524s\n",
      "Epoch  88 Average Loss:  0.294 Epoch duration: 49.030s\n",
      "Epoch  89 Average Loss:  0.294 Epoch duration: 48.107s\n",
      "Epoch  90 Average Loss:  0.293 Epoch duration: 47.703s\n",
      "Epoch  91 Average Loss:  0.293 Epoch duration: 47.660s\n",
      "Epoch  92 Average Loss:  0.293 Epoch duration: 47.988s\n",
      "Epoch  93 Average Loss:  0.293 Epoch duration: 48.008s\n",
      "Epoch  94 Average Loss:  0.293 Epoch duration: 47.686s\n",
      "Epoch  95 Average Loss:  0.293 Epoch duration: 49.168s\n",
      "Epoch  96 Average Loss:  0.293 Epoch duration: 47.187s\n",
      "Epoch  97 Average Loss:  0.293 Epoch duration: 47.318s\n",
      "Epoch  98 Average Loss:  0.293 Epoch duration: 47.118s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2515 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  99 Average Loss:  0.292 Epoch duration: 47.046s\n",
      "Total training time: 4800.203766345978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2515/2515 [00:48<00:00, 50.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test loss: 0.30878183\n",
      "\n",
      "Test sentence: an old pike head sir there were seams dents in it\n",
      "Prediction: {'🈁', '4', '🤕'}\n",
      "Cosine distance: 0.22518182\n",
      "\n",
      "Test sentence: this one poor hunt then the best lance out all surely he will not hang back when every foremast hand has clutched whetstone\n",
      "Prediction: {'🔂', '🆙', '🈁', '4', '⚜', '😰'}\n",
      "Cosine distance: 0.12840074\n",
      "\n",
      "Test sentence: drop them over fore aft\n",
      "Prediction: {'⚜', '4'}\n",
      "Cosine distance: 0.3641491\n",
      "\n",
      "Test sentence: in the infancy the first settlement the emigrants were several times saved from starvation by the benevolent biscuit the whale ship luckily dropping an anchor in their waters\n",
      "Prediction: {'🏤', '🤒', '🍠', '♌', '🈴', '🆘', '⛵', '🈁', '🕦', '💯', '4', '🍞'}\n",
      "Cosine distance: 0.16920751\n",
      "\n",
      "Test sentence: mighty whales which swim in sea water have sea oil swimming in them\n",
      "Prediction: {'⛽', '♓', '⛵', '🥘', '4'}\n",
      "Cosine distance: 0.17872876\n",
      "\n",
      "Test sentence: round round the fish s back pinioned in the turns upon turns in which during the past night the whale had reeled the the lines around him the half torn body the\n",
      "Prediction: {'〽', '💓', '👁', '🆙', '♨', '🕜', '4', '🍞'}\n",
      "Cosine distance: 0.1461479\n",
      "\n",
      "Test sentence: it is mild mild wind mild looking sky\n",
      "Prediction: {'🌖', '🈁', '4', '🐌'}\n",
      "Cosine distance: 0.33529598\n",
      "\n",
      "Test sentence: large thorough sweeping comprehension him it behooves me now unbutton him still further the points his hose unbuckling his garters casting loose the hooks the eyes the joints his innermost bones set\n",
      "Prediction: {'🚫', '🙏', '🐌', '🈴', '🏻', '♨', '🥘', '♎', '😰', '4'}\n",
      "Cosine distance: 0.17253\n",
      "\n",
      "Test sentence: could not fasten\n",
      "Prediction: {'🙏', '4'}\n",
      "Cosine distance: 0.39157176\n",
      "\n",
      "Test sentence: you this\n",
      "Prediction: {'🈁', '4'}\n",
      "Cosine distance: 0.30360812\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "epochs = 100\n",
    "start = time.time()\n",
    "\n",
    "for i in range(epochs):\n",
    "    t = time.time()\n",
    "    losses = []\n",
    "    for X in utils.batch_generator(X_train, batch_size):\n",
    "        _, l, summary = sess.run([optimizer, loss, merged], feed_dict={inputs:X})\n",
    "        losses.append(l)\n",
    "    writer.add_summary(summary, global_step=i)\n",
    "#     if l < .0005:\n",
    "#         print('Epoch {:3} Loss: {:>6.3f} Epoch duration: {:>6.3f}s'.format(i, l, time.time() - t))\n",
    "#         break\n",
    "#     elif not i%10:\n",
    "    print('Epoch {:3} Average Loss: {:>6.3f} Epoch duration: {:>6.3f}s'.format(i, np.mean(losses, axis=-1), time.time() - t))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, 'models/seq2seq_moby_dick/model1')\n",
    "print('Total training time:', time.time()-start)\n",
    "\n",
    "predictions = []\n",
    "losses = []\n",
    "emoji_keys = list(emoji_vectors.keys())\n",
    "for x in tqdm(X_test):\n",
    "    lo, l = sess.run([logits, loss], feed_dict={inputs:np.array(x).reshape(-1, 32, 300)})\n",
    "    pred = np.argmax(lo, axis=2).reshape(32,)\n",
    "    predictions.append([emoji_keys[i] for i in pred])\n",
    "    losses.append(l)\n",
    "\n",
    "print('Average test loss:', np.mean(losses, axis=-1))\n",
    "print()\n",
    "\n",
    "for i in range(10):\n",
    "    print('Test sentence:', ' '.join(w for w in test_sents[i] if w))\n",
    "    print('Prediction:', set(predictions[i]))\n",
    "    print('Cosine distance:', losses[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2515/2515 [00:29<00:00, 86.02it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "losses = []\n",
    "emoji_keys = list(emoji_vectors.keys())\n",
    "for x in tqdm(X_test):\n",
    "    lo, l = sess.run([logits, loss], feed_dict={inputs:np.array(x).reshape(-1, 32, 300)})\n",
    "    pred = np.argmax(lo, axis=2).reshape(32,)\n",
    "    predictions.append([emoji_keys[i] for i in pred])\n",
    "    losses.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sentence: an old pike head sir there were seams dents in it\n",
      "Prediction: {'🤕', '🈁', '♈', '🐓'}\n",
      "Cosine distance: 0.2244193\n",
      "\n",
      "Test sentence: this one poor hunt then the best lance out all surely he will not hang back when every foremast hand has clutched whetstone\n",
      "Prediction: {'⚛', '🆙', '🈁', '🙏', '🔂', '⚜', '♈', '🐓'}\n",
      "Cosine distance: 0.13012469\n",
      "\n",
      "Test sentence: drop them over fore aft\n",
      "Prediction: {'⚜', '♈', '🐓', '🆙'}\n",
      "Cosine distance: 0.36446148\n",
      "\n",
      "Test sentence: in the infancy the first settlement the emigrants were several times saved from starvation by the benevolent biscuit the whale ship luckily dropping an anchor in their waters\n",
      "Prediction: {'♏', '🛳', '🆙', '🍠', '🈁', '🈴', '♈', '⚰', '⛵', '〽', '🐓', '🔖'}\n",
      "Cosine distance: 0.16519237\n",
      "\n",
      "Test sentence: mighty whales which swim in sea water have sea oil swimming in them\n",
      "Prediction: {'🐋', '♈', '⛽', '🤼', '🌊', '🐓'}\n",
      "Cosine distance: 0.14461929\n",
      "\n",
      "Test sentence: round round the fish s back pinioned in the turns upon turns in which during the past night the whale had reeled the the lines around him the half torn body the\n",
      "Prediction: {'👂', '⚛', '🌉', '🆙', '🕦', '🈁', '♈', '♨', '🕜', '〽', '🐓', '🔖'}\n",
      "Cosine distance: 0.15253574\n",
      "\n",
      "Test sentence: it is mild mild wind mild looking sky\n",
      "Prediction: {'♈', '🐓'}\n",
      "Cosine distance: 0.31968683\n",
      "\n",
      "Test sentence: large thorough sweeping comprehension him it behooves me now unbutton him still further the points his hose unbuckling his garters casting loose the hooks the eyes the joints his innermost bones set\n",
      "Prediction: {'🥄', '😰', '👖', '⚛', '🆙', '🈁', '🙏', '🉑', '🈴', '🚊', '🍗', '♈', '⛽', '♨', '💯', '🐓', '💤'}\n",
      "Cosine distance: 0.16600806\n",
      "\n",
      "Test sentence: could not fasten\n",
      "Prediction: {'♈', '🐓', '🙏'}\n",
      "Cosine distance: 0.37212747\n",
      "\n",
      "Test sentence: you this\n",
      "Prediction: {'♈', '🐓'}\n",
      "Cosine distance: 0.3108195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('Test sentence:', ' '.join(w for w in test_sents[i] if w))\n",
    "    print('Prediction:', set(predictions[i]))\n",
    "    print('Cosine distance:', losses[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
