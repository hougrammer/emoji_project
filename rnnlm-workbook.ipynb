{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network Language Model\n",
    "\n",
    "This is the \"working notebook\", with skeleton code to load and train your model, as well as run unit tests. See [rnnlm-instructions.ipynb](rnnlm-instructions.ipynb) for the main writeup.\n",
    "\n",
    "Run the cell below to import packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rnnlm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ed1a92785b1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Your code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mrnnlm\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnnlm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrnnlm_test\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnnlm_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rnnlm'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "#from w266_common import utils, vocabulary, tf_embed_viz\n",
    "import utils\n",
    "import vocabulary\n",
    "\n",
    "# Your code\n",
    "import rnnlm; reload(rnnlm)\n",
    "import rnnlm_test; reload(rnnlm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "#from w266_common import utils, vocabulary, tf_embed_viz\n",
    "import utils\n",
    "import vocabulary\n",
    "\n",
    "# Your code\n",
    "import rnnlm; reload(rnnlm)\n",
    "import rnnlm_test; reload(rnnlm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Implementing the RNNLM\n",
    "\n",
    "### Aside: Shapes Review\n",
    "\n",
    "Before we start, let's review our understanding of the shapes involved in this assignment and how they manifest themselves in the TF API.\n",
    "\n",
    "As in the [instructions](rnnlm-instructions.ipynb) notebook, $w$ is a matrix of wordids with shape batch_size x max_time.  Passing this through the embedding layer, we retrieve the word embedding for each, resulting in $x$ having shape batch_size x max_time x embedding_dim.  I find it useful to draw this out on a piece of paper.  When you do, you should end up with a rectangular prism with batch_size height, max_time width and embedding_dim depth.  Many tensors in this assignment share this shape (e.g. $o$, the output from the LSTM, which represents the hidden layer going into the softmax to make a prediction at every time step in every batch).\n",
    "\n",
    "![Three Dimensional Shape](common_shape.png)\n",
    "\n",
    "Since batch size and sentence length are only resolved when we run the graph, we construct the placeholder using \"None\" in the dimensions we don't know.  The .shape property renders these as ?s.  This should be familiar to you from batch size handling in earlier assignments, only now there are two dimensions of variable length.\n",
    "\n",
    "See the next cell for a concrete example (though in practice, we'd use a TensorFlow variable that we can train for the embeddings rather than a static array).  Notice how the shape of x_val matches the shape described earlier in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordid placeholder shape: (?, ?)\n",
      "x shape: (?, ?, 3)\n",
      "Embeddings shape: (2, 4, 3)\n",
      "Embeddings value:\n",
      " [[[2 2 2]\n",
      "  [3 3 3]\n",
      "  [2 2 2]\n",
      "  [3 3 3]]\n",
      "\n",
      " [[1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]\n",
      "  [1 1 1]]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "wordid_ph = tf.placeholder(tf.int32, shape=[None, None])\n",
    "embedding_matrix = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\n",
    "x = tf.nn.embedding_lookup(embedding_matrix, wordid_ph)\n",
    "\n",
    "print('wordid placeholder shape:', wordid_ph.shape)\n",
    "print('x shape:', x.shape)\n",
    "\n",
    "sess = tf.Session()\n",
    "# Two sentences, each with four words.\n",
    "wordids = [[1, 2, 1, 2], [0, 0, 0, 0]]\n",
    "x_val = sess.run(x, feed_dict={wordid_ph: wordids})\n",
    "print('Embeddings shape:', x_val.shape)  # 2 sentences, 4 words, \n",
    "print('Embeddings value:\\n', x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implmenting the RNNLM\n",
    "\n",
    "In order to better manage the model parameters, we'll implement our RNNLM in the `RNNLM` class in `rnnlm.py`. We've given you a skeleton of starter code for this, but the bulk of the implementation is left to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(rnnlm)\n",
    "\n",
    "TF_GRAPHDIR = \"/tmp/w266/a3_graph\"\n",
    "\n",
    "# Clear old log directory.\n",
    "shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(V=10000, H=200, num_layers=2)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above will load your implementation, construct the graph, and write a logdir for TensorBoard. You can bring up TensorBoard with:\n",
    "```\n",
    "cd assignment/a3\n",
    "tensorboard --logdir /tmp/w266/a3_graph --port 6006\n",
    "```\n",
    "As usual, check http://localhost:6006/ and visit the \"Graphs\" tab to inspect your implementation. Remember, judicious use of `tf.name_scope()` and/or `tf.variable_scope()` will greatly improve the visualization, and make code easier to debug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've provided a few unit tests below to verify some *very* basic properties of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_shapes_embed (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_output (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_recurrent (rnnlm_test.TestRNNLMCore) ... ok\n",
      "test_shapes_train (rnnlm_test.TestRNNLMTrain) ... ok\n",
      "test_shapes_sample (rnnlm_test.TestRNNLMSampler) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 1.993s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "reload(rnnlm); reload(rnnlm_test)\n",
    "utils.run_tests(rnnlm_test, [\"TestRNNLMCore\", \"TestRNNLMTrain\", \"TestRNNLMSampler\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the error messages are intentionally somewhat spare, and that passing tests are no guarantee of model correctness! Your best chance of success is through careful coding and understanding of how the model works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Training your RNNLM (5 points)\n",
    "\n",
    "We'll give you data loader functions in **`utils.py`**. They work similarly to the loaders in the Week 5 notebook.\n",
    "\n",
    "Particularly, `utils.rnnlm_batch_generator` will return an iterator that yields minibatches in the correct format. Batches will be of size `[batch_size, max_time]`, and consecutive batches will line up along rows so that the final state $h^{\\text{final}}$ of one batch can be used as the initial state $h^{\\text{init}}$ for the next.\n",
    "\n",
    "For example, using a toy corpus:  \n",
    "*(Ignore the ugly formatter code.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Input words w:</h3><table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "      <th>w_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_0</th>\n",
       "      <th>w_1</th>\n",
       "      <th>w_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>little</td>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>white</td>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Target words y:</h3><table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "      <th>y_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mary</td>\n",
       "      <td>had</td>\n",
       "      <td>a</td>\n",
       "      <td>little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The</td>\n",
       "      <td>lamb</td>\n",
       "      <td>was</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td><td><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_0</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lamb</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>as</td>\n",
       "      <td>snow</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toy_corpus = \"<s> Mary had a little lamb . <s> The lamb was white as snow . <s>\"\n",
    "toy_corpus = np.array(toy_corpus.split())\n",
    "\n",
    "html = \"<h3>Input words w:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "bi = utils.rnnlm_batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    cols = [\"w_%d\" % d for d in range(w.shape[1])]\n",
    "    html += \"<td>{:s}</td>\".format(utils.render_matrix(w, cols=cols, dtype=object))\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))\n",
    "\n",
    "html = \"<h3>Target words y:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "bi = utils.rnnlm_batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    cols = [\"y_%d\" % d for d in range(y.shape[1])]\n",
    "    html += \"<td>{:s}</td>\".format(utils.render_matrix(y, cols=cols, dtype=object))\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data we feed to our model will be word indices, but the shape will be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement the `run_epoch` function\n",
    "We've given you some starter code for logging progress; fill this in with actual call(s) to `session.run` with the appropriate arguments to run a training step. \n",
    "\n",
    "Be sure to handle the initial state properly at the beginning of an epoch, and remember to carry over the final state from each batch and use it as the initial state for the next.\n",
    "\n",
    "**Note:** we provide a `train=True` flag to enable train mode. If `train=False`, this function can also be used for scoring the dataset - see `score_dataset()` below.\n",
    "\n",
    "#### Questions\n",
    "\n",
    "1.  Explain what this function does.  Be sure to include the role of `batch_iterator` and what's going on with `h` in the inner loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=None):\n",
    "    assert(learning_rate is not None)\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        feed_dict = {\n",
    "            lm.input_w_: w,\n",
    "            lm.target_y_: y,\n",
    "            lm.initial_h_: h,\n",
    "            lm.learning_rate_: learning_rate,\n",
    "            lm.use_dropout_: use_dropout\n",
    "        }\n",
    "        ops = [loss, lm.final_h_, train_op]        \n",
    "        #### YOUR CODE HERE ####\n",
    "        # session.run(...) the ops with the feed_dict constructed above.\n",
    "        # Ensure \"cost\" becomes the value of \"loss\".\n",
    "        # Hint: see \"ops\" for other variables that need updating in this loop.\n",
    "        cost, h, _ = session.run(ops, feed_dict)\n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print(\"[batch {:d}]: seen {:d} words at {:.1f} wps, loss = {:.3f}\".format(\n",
    "                i, total_words, avg_wps, avg_cost))\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.rnnlm_batch_generator(ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=0.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print(\"{:s}: avg. loss: {:.03f}  (perplexity: {:.02f})\".format(name, cost, np.exp(cost)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the cell below to verify your implementation of `run_epoch`, and to test your RNN on a (very simple) toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_toy_model (rnnlm_test.RunEpochTester) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch 65]: seen 3300 words at 3285.5 wps, loss = 1.055\n",
      "[batch 160]: seen 8050 words at 4007.7 wps, loss = 0.535\n",
      "[batch 256]: seen 12850 words at 4269.0 wps, loss = 0.366\n",
      "[batch 354]: seen 17750 words at 4424.0 wps, loss = 0.284\n",
      "[batch 452]: seen 22650 words at 4513.4 wps, loss = 0.234\n",
      "[batch 546]: seen 27350 words at 4541.8 wps, loss = 0.204\n",
      "[batch 641]: seen 32100 words at 4564.3 wps, loss = 0.181\n",
      "[batch 739]: seen 37000 words at 4602.1 wps, loss = 0.164\n",
      "[batch 838]: seen 41950 words at 4639.1 wps, loss = 0.149\n",
      "[batch 935]: seen 46800 words at 4655.8 wps, loss = 0.138\n",
      "[batch 1031]: seen 51600 words at 4665.8 wps, loss = 0.130\n",
      "[batch 1127]: seen 56400 words at 4673.8 wps, loss = 0.122\n",
      "[batch 1222]: seen 61150 words at 4679.1 wps, loss = 0.116\n",
      "[batch 1315]: seen 65800 words at 4676.7 wps, loss = 0.111\n",
      "[batch 1412]: seen 70650 words at 4685.8 wps, loss = 0.106\n",
      "[batch 1508]: seen 75450 words at 4692.7 wps, loss = 0.102\n",
      "[batch 1603]: seen 80200 words at 4695.5 wps, loss = 0.098\n",
      "[batch 1698]: seen 84950 words at 4696.0 wps, loss = 0.095\n",
      "[batch 1793]: seen 89700 words at 4696.8 wps, loss = 0.092\n",
      "[batch 1891]: seen 94600 words at 4706.8 wps, loss = 0.089\n",
      "[batch 1991]: seen 99600 words at 4719.6 wps, loss = 0.086\n",
      "[batch 2089]: seen 104500 words at 4725.9 wps, loss = 0.084\n",
      "[batch 2187]: seen 109400 words at 4732.8 wps, loss = 0.082\n",
      "[batch 2282]: seen 114150 words at 4731.8 wps, loss = 0.080\n",
      "[batch 2377]: seen 118900 words at 4732.2 wps, loss = 0.078\n",
      "[batch 2472]: seen 123650 words at 4731.0 wps, loss = 0.077\n",
      "[batch 2569]: seen 128500 words at 4735.0 wps, loss = 0.075\n",
      "[batch 2670]: seen 133550 words at 4744.8 wps, loss = 0.074\n",
      "[batch 2771]: seen 138600 words at 4754.2 wps, loss = 0.072\n",
      "[batch 2866]: seen 143350 words at 4753.6 wps, loss = 0.071\n",
      "[batch 2961]: seen 148100 words at 4752.7 wps, loss = 0.070\n",
      "[batch 3060]: seen 153050 words at 4758.3 wps, loss = 0.069\n",
      "[batch 3157]: seen 157900 words at 4761.0 wps, loss = 0.068\n",
      "[batch 3253]: seen 162700 words at 4760.8 wps, loss = 0.067\n",
      "[batch 3346]: seen 167350 words at 4757.5 wps, loss = 0.066\n",
      "[batch 3442]: seen 172150 words at 4758.2 wps, loss = 0.065\n",
      "[batch 3540]: seen 177050 words at 4761.4 wps, loss = 0.064\n",
      "[batch 3635]: seen 181800 words at 4759.9 wps, loss = 0.063\n",
      "[batch 3730]: seen 186550 words at 4759.3 wps, loss = 0.062\n",
      "[batch 3825]: seen 191300 words at 4758.9 wps, loss = 0.062\n",
      "[batch 3921]: seen 196100 words at 4759.4 wps, loss = 0.061\n",
      "Train set: avg. loss: 0.000  (perplexity: 1.00)\n",
      "Test set: avg. loss: 0.001  (perplexity: 1.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 43.735s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rnnlm); reload(rnnlm_test)\n",
    "th = rnnlm_test.RunEpochTester(\"test_toy_model\")\n",
    "th.setUp(); th.injectCode(run_epoch, score_dataset)\n",
    "unittest.TextTestRunner(verbosity=2).run(th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as above, this is a *very* simple test case that does not guarantee model correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run Training\n",
    "\n",
    "We'll give you the outline of the training procedure, but you'll need to fill in a call to your `run_epoch` function. \n",
    "\n",
    "At the end of training, we use a `tf.train.Saver` to save a copy of the model to `/tmp/w266/a3_model/rnnlm_trained`. You'll want to load this from disk to work on later parts of the assignment; see **part (d)** for an example of how this is done.\n",
    "\n",
    "#### Tuning Hyperparameters\n",
    "With a sampled softmax loss, the default hyperparameters should train 5 epochs in around 15 minutes on a single-core GCE instance, and reach a training set perplexity between 120-140.\n",
    "\n",
    "However, it's possible to do significantly better. Try experimenting with multiple RNN layers (`num_layers` > 1) or a larger hidden state - though you may also need to adjust the learning rate and number of epochs for a larger model.\n",
    "\n",
    "You can also experiment with a larger vocabulary. This will look worse for perplexity, but will be a better model overall as it won't treat so many words as `<unk>`.\n",
    "\n",
    "#### Notes on Speed\n",
    "\n",
    "To speed things up, you may want to re-start your GCE instance with more CPUs. Using a 16-core machine will train *very* quickly if using a sampled softmax lost, almost as fast as a GPU. (Because of the sequential nature of the model, GPUs aren't actually much faster than CPUs for training and running RNNs.) The training code will print the words-per-second processed; with the default settings on a single core, you can expect around 8000 WPS, or up to more than 25000 WPS on a fast multi-core machine.\n",
    "\n",
    "You might also want to modify the code below to only run score_dataset at the very end, after all epochs are completed. This will speed things up significantly, since `score_dataset` uses the full softmax loss - and so often can take longer than a whole training epoch!\n",
    "\n",
    "#### Submitting your model\n",
    "You should submit your trained model along with the assignment. Do:\n",
    "```\n",
    "cp /tmp/w266/a3_model/rnnlm_trained* .\n",
    "git add rnnlm_trained*\n",
    "git commit -m \"Adding trained model.\"\n",
    "```\n",
    "Unless you train a very large model, these files should be < 50 MB and no problem for git to handle. If you do also train a large model, please only submit the smaller one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'utils' has no attribute 'load_corpus'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f038152f3212>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"brown\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'utils' has no attribute 'load_corpus'"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "V = 10000\n",
    "vocab, train_ids, test_ids = utils.load_corpus(\"brown\", split=0.8, V=V, shuffle=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 25\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, \n",
    "                    H=200, \n",
    "                    softmax_ns=200,\n",
    "                    num_layers=2)\n",
    "\n",
    "TF_SAVEDIR = \"/tmp/w266/a3_model\"\n",
    "# tensorboard --logdir=/tmp/w266/a3_model\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n",
      "[batch 32]: seen 82500 words at 16270.6 wps, loss = 6.406\n",
      "[batch 67]: seen 170000 words at 16772.2 wps, loss = 5.746\n",
      "[batch 102]: seen 257500 words at 17006.8 wps, loss = 5.427\n",
      "[batch 138]: seen 347500 words at 17158.1 wps, loss = 5.231\n",
      "[batch 174]: seen 437500 words at 17241.2 wps, loss = 5.099\n",
      "[batch 210]: seen 527500 words at 17291.3 wps, loss = 5.000\n",
      "[batch 246]: seen 617500 words at 17341.2 wps, loss = 4.921\n",
      "[batch 282]: seen 707500 words at 17362.5 wps, loss = 4.854\n",
      "[batch 318]: seen 797500 words at 17385.7 wps, loss = 4.801\n",
      "[batch 354]: seen 887500 words at 17407.9 wps, loss = 4.753\n",
      "[epoch 1] Completed in 0:00:55\n",
      "[epoch 2] Starting epoch 2\n",
      "[batch 35]: seen 90000 words at 17829.2 wps, loss = 5.702\n",
      "[batch 71]: seen 180000 words at 17842.7 wps, loss = 5.506\n",
      "[batch 107]: seen 270000 words at 17816.9 wps, loss = 5.343\n",
      "[batch 143]: seen 360000 words at 17806.8 wps, loss = 5.073\n",
      "[batch 179]: seen 450000 words at 17795.4 wps, loss = 4.895\n",
      "[batch 215]: seen 540000 words at 17783.1 wps, loss = 4.771\n",
      "[batch 251]: seen 630000 words at 17782.2 wps, loss = 4.682\n",
      "[batch 287]: seen 720000 words at 17784.6 wps, loss = 4.610\n",
      "[batch 323]: seen 810000 words at 17792.8 wps, loss = 4.552\n",
      "[batch 359]: seen 900000 words at 17793.9 wps, loss = 4.505\n",
      "[epoch 2] Completed in 0:00:54\n",
      "[epoch 3] Starting epoch 3\n",
      "[batch 35]: seen 90000 words at 17886.7 wps, loss = 4.061\n",
      "[batch 71]: seen 180000 words at 17931.7 wps, loss = 4.048\n",
      "[batch 107]: seen 270000 words at 17919.4 wps, loss = 4.039\n",
      "[batch 143]: seen 360000 words at 17909.1 wps, loss = 4.033\n",
      "[batch 179]: seen 450000 words at 17876.4 wps, loss = 4.024\n",
      "[batch 215]: seen 540000 words at 17881.5 wps, loss = 4.019\n",
      "[batch 251]: seen 630000 words at 17883.5 wps, loss = 4.015\n",
      "[batch 287]: seen 720000 words at 17886.8 wps, loss = 4.009\n",
      "[batch 323]: seen 810000 words at 17885.4 wps, loss = 4.004\n",
      "[batch 359]: seen 900000 words at 17888.8 wps, loss = 3.999\n",
      "[epoch 3] Completed in 0:00:54\n",
      "[epoch 4] Starting epoch 4\n",
      "[batch 35]: seen 90000 words at 17916.9 wps, loss = 3.959\n",
      "[batch 71]: seen 180000 words at 17950.5 wps, loss = 3.939\n",
      "[batch 108]: seen 272500 words at 17969.8 wps, loss = 3.933\n",
      "[batch 145]: seen 365000 words at 17982.0 wps, loss = 3.934\n",
      "[batch 182]: seen 457500 words at 17989.6 wps, loss = 3.927\n",
      "[batch 219]: seen 550000 words at 18009.9 wps, loss = 3.921\n",
      "[batch 255]: seen 640000 words at 17997.4 wps, loss = 3.916\n",
      "[batch 291]: seen 730000 words at 17988.9 wps, loss = 3.914\n",
      "[batch 328]: seen 822500 words at 17998.9 wps, loss = 3.909\n",
      "[batch 364]: seen 912500 words at 17998.1 wps, loss = 3.905\n",
      "[epoch 4] Completed in 0:00:53\n",
      "[epoch 5] Starting epoch 5\n",
      "[batch 34]: seen 87500 words at 17203.4 wps, loss = 3.869\n",
      "[batch 66]: seen 167500 words at 16485.4 wps, loss = 3.860\n",
      "[batch 99]: seen 250000 words at 16373.8 wps, loss = 3.853\n",
      "[batch 133]: seen 335000 words at 16487.2 wps, loss = 3.853\n",
      "[batch 170]: seen 427500 words at 16810.9 wps, loss = 3.851\n",
      "[batch 206]: seen 517500 words at 17002.6 wps, loss = 3.849\n",
      "[batch 243]: seen 610000 words at 17165.9 wps, loss = 3.848\n",
      "[batch 280]: seen 702500 words at 17283.5 wps, loss = 3.846\n",
      "[batch 317]: seen 795000 words at 17374.3 wps, loss = 3.845\n",
      "[batch 354]: seen 887500 words at 17450.9 wps, loss = 3.842\n",
      "[epoch 5] Completed in 0:00:55\n",
      "[epoch 6] Starting epoch 6\n",
      "[batch 36]: seen 92500 words at 18035.1 wps, loss = 3.821\n",
      "[batch 73]: seen 185000 words at 18033.5 wps, loss = 3.811\n",
      "[batch 110]: seen 277500 words at 18047.9 wps, loss = 3.808\n",
      "[batch 147]: seen 370000 words at 18041.8 wps, loss = 3.808\n",
      "[batch 183]: seen 460000 words at 18018.6 wps, loss = 3.805\n",
      "[batch 220]: seen 552500 words at 18025.1 wps, loss = 3.803\n",
      "[batch 257]: seen 645000 words at 18024.0 wps, loss = 3.801\n",
      "[batch 293]: seen 735000 words at 18017.5 wps, loss = 3.800\n",
      "[batch 329]: seen 825000 words at 18014.5 wps, loss = 3.798\n",
      "[batch 366]: seen 917500 words at 18014.0 wps, loss = 3.797\n",
      "[epoch 6] Completed in 0:00:53\n",
      "[epoch 7] Starting epoch 7\n",
      "[batch 35]: seen 90000 words at 17686.3 wps, loss = 3.778\n",
      "[batch 71]: seen 180000 words at 17696.7 wps, loss = 3.773\n",
      "[batch 107]: seen 270000 words at 17703.8 wps, loss = 3.770\n",
      "[batch 143]: seen 360000 words at 17711.2 wps, loss = 3.771\n",
      "[batch 179]: seen 450000 words at 17723.5 wps, loss = 3.770\n",
      "[batch 215]: seen 540000 words at 17727.2 wps, loss = 3.767\n",
      "[batch 251]: seen 630000 words at 17744.4 wps, loss = 3.765\n",
      "[batch 287]: seen 720000 words at 17743.7 wps, loss = 3.765\n",
      "[batch 323]: seen 810000 words at 17744.0 wps, loss = 3.764\n",
      "[batch 359]: seen 900000 words at 17742.5 wps, loss = 3.764\n",
      "[epoch 7] Completed in 0:00:54\n",
      "[epoch 8] Starting epoch 8\n",
      "[batch 35]: seen 90000 words at 17867.5 wps, loss = 3.751\n",
      "[batch 71]: seen 180000 words at 17844.9 wps, loss = 3.744\n",
      "[batch 107]: seen 270000 words at 17820.4 wps, loss = 3.738\n",
      "[batch 143]: seen 360000 words at 17811.3 wps, loss = 3.740\n",
      "[batch 179]: seen 450000 words at 17779.7 wps, loss = 3.739\n",
      "[batch 215]: seen 540000 words at 17792.2 wps, loss = 3.737\n",
      "[batch 251]: seen 630000 words at 17773.1 wps, loss = 3.735\n",
      "[batch 287]: seen 720000 words at 17785.6 wps, loss = 3.735\n",
      "[batch 323]: seen 810000 words at 17792.5 wps, loss = 3.734\n",
      "[batch 359]: seen 900000 words at 17789.2 wps, loss = 3.733\n",
      "[epoch 8] Completed in 0:00:54\n",
      "[epoch 9] Starting epoch 9\n",
      "[batch 35]: seen 90000 words at 17766.3 wps, loss = 3.723\n",
      "[batch 71]: seen 180000 words at 17807.7 wps, loss = 3.716\n",
      "[batch 107]: seen 270000 words at 17794.8 wps, loss = 3.712\n",
      "[batch 143]: seen 360000 words at 17793.9 wps, loss = 3.713\n",
      "[batch 179]: seen 450000 words at 17819.2 wps, loss = 3.712\n",
      "[batch 215]: seen 540000 words at 17820.3 wps, loss = 3.710\n",
      "[batch 251]: seen 630000 words at 17834.3 wps, loss = 3.708\n",
      "[batch 287]: seen 720000 words at 17839.3 wps, loss = 3.708\n",
      "[batch 323]: seen 810000 words at 17833.5 wps, loss = 3.708\n",
      "[batch 359]: seen 900000 words at 17823.7 wps, loss = 3.707\n",
      "[epoch 9] Completed in 0:00:54\n",
      "[epoch 10] Starting epoch 10\n",
      "[batch 35]: seen 90000 words at 17921.6 wps, loss = 3.701\n",
      "[batch 71]: seen 180000 words at 17937.8 wps, loss = 3.691\n",
      "[batch 107]: seen 270000 words at 17942.1 wps, loss = 3.690\n",
      "[batch 143]: seen 360000 words at 17899.4 wps, loss = 3.691\n",
      "[batch 179]: seen 450000 words at 17890.0 wps, loss = 3.689\n",
      "[batch 215]: seen 540000 words at 17893.0 wps, loss = 3.687\n",
      "[batch 251]: seen 630000 words at 17886.9 wps, loss = 3.686\n",
      "[batch 287]: seen 720000 words at 17879.0 wps, loss = 3.686\n",
      "[batch 319]: seen 800000 words at 17648.9 wps, loss = 3.686\n",
      "[batch 351]: seen 880000 words at 17468.5 wps, loss = 3.685\n",
      "[batch 385]: seen 965000 words at 17392.1 wps, loss = 3.685\n",
      "[epoch 10] Completed in 0:00:55\n",
      "[epoch 10] Train set: avg. loss: 4.868  (perplexity: 130.11)\n",
      "[epoch 10] Test set: avg. loss: 4.985  (perplexity: 146.16)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = utils.rnnlm_batch_generator(train_ids, batch_size, max_time)\n",
    "        print(\"[epoch {:d}] Starting epoch {:d}\".format(epoch, epoch))\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Run a training epoch.\n",
    "        run_epoch(lm, session, bi, train=True, verbose=True, tick_s=print_interval, learning_rate=learning_rate)\n",
    "        \n",
    "        #### END(YOUR CODE) ####\n",
    "        print(\"[epoch {:d}] Completed in {:s}\".format(epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "    print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "    score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "    print(\"[epoch {:d}]\".format(epoch), end=\" \")\n",
    "    score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Sampling Sentences (5 points)\n",
    "\n",
    "If you didn't already in **part (b)**, implement the `BuildSamplerGraph()` method in `rnnlm.py` See the function docstring for more information.\n",
    "\n",
    "#### Implement the `sample_step()` method below (5 points)\n",
    "This should access the Tensors you create in `BuildSamplerGraph()`. Given an input batch and initial states, it should return a vector of shape `[batch_size,1]` containing sampled indices for the next word of each batch sequence.\n",
    "\n",
    "Run the method using the provided code to generate 10 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_step(lm, session, input_w, initial_h):\n",
    "    \"\"\"Run a single RNN step and return sampled predictions.\n",
    "  \n",
    "    Args:\n",
    "      lm : rnnlm.RNNLM\n",
    "      session: tf.Session\n",
    "      input_w : [batch_size] vector of indices\n",
    "      initial_h : [batch_size, hidden_dims] initial state\n",
    "    \n",
    "    Returns:\n",
    "      final_h : final hidden state, compatible with initial_h\n",
    "      samples : [batch_size, 1] vector of indices\n",
    "    \"\"\"\n",
    "    # Reshape input to column vector\n",
    "    input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
    "  \n",
    "    #### YOUR CODE HERE ####\n",
    "    # Run sample ops\n",
    "    ops = [lm.final_h_, lm.pred_samples_]\n",
    "    feed_dict = {lm.initial_h_: initial_h, lm.input_w_: w}\n",
    "    \n",
    "    final_h, samples = session.run(ops, feed_dict)\n",
    "\n",
    "    #### END(YOUR CODE) ####\n",
    "    # Note indexing here: \n",
    "    #   [batch_size, max_time, 1] -> [batch_size, 1]\n",
    "    print(samples[0])\n",
    "    return final_h, samples[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from rnnlm_trained\n",
      "[[4540]]\n",
      "[[1217]\n",
      " [2572]]\n",
      "[[ 174]\n",
      " [2057]\n",
      " [  10]]\n",
      "[[ 68]\n",
      " [190]\n",
      " [ 59]\n",
      " [194]]\n",
      "[[   7]\n",
      " [   4]\n",
      " [ 216]\n",
      " [  39]\n",
      " [2380]]\n",
      "[[7186]\n",
      " [   8]\n",
      " [  18]\n",
      " [   3]\n",
      " [3645]\n",
      " [ 945]]\n",
      "[[ 12]\n",
      " [  2]\n",
      " [ 18]\n",
      " [  3]\n",
      " [580]\n",
      " [  2]\n",
      " [  6]]\n",
      "[[   3]\n",
      " [   5]\n",
      " [   5]\n",
      " [   3]\n",
      " [ 650]\n",
      " [5705]\n",
      " [   8]\n",
      " [5300]]\n",
      "[[5009]\n",
      " [3375]\n",
      " [   3]\n",
      " [   9]\n",
      " [ 934]\n",
      " [3522]\n",
      " [  24]\n",
      " [ 743]\n",
      " [   4]]\n",
      "[[3541]\n",
      " [   2]\n",
      " [   3]\n",
      " [  28]\n",
      " [1455]\n",
      " [ 406]\n",
      " [   6]\n",
      " [  39]\n",
      " [   4]\n",
      " [  15]]\n",
      "[[  57]\n",
      " [   0]\n",
      " [ 460]\n",
      " [   3]\n",
      " [4095]\n",
      " [ 751]\n",
      " [   6]\n",
      " [1071]\n",
      " [   8]\n",
      " [2153]\n",
      " [  45]]\n",
      "[[  27]\n",
      " [4263]\n",
      " [   9]\n",
      " [ 141]\n",
      " [   2]\n",
      " [1397]\n",
      " [  20]\n",
      " [   3]\n",
      " [   5]\n",
      " [  35]\n",
      " [   9]\n",
      " [   3]]\n",
      "[[  61]\n",
      " [1404]\n",
      " [4324]\n",
      " [   3]\n",
      " [ 922]\n",
      " [ 544]\n",
      " [  10]\n",
      " [   2]\n",
      " [   8]\n",
      " [   7]\n",
      " [   3]\n",
      " [  41]\n",
      " [   2]]\n",
      "[[  12]\n",
      " [1000]\n",
      " [8633]\n",
      " [   9]\n",
      " [ 969]\n",
      " [4168]\n",
      " [ 282]\n",
      " [ 760]\n",
      " [  35]\n",
      " [  35]\n",
      " [   3]\n",
      " [   4]\n",
      " [   2]\n",
      " [2600]]\n",
      "[[  11]\n",
      " [3739]\n",
      " [   4]\n",
      " [  21]\n",
      " [2006]\n",
      " [2013]\n",
      " [ 533]\n",
      " [   2]\n",
      " [   5]\n",
      " [   2]\n",
      " [ 350]\n",
      " [1290]\n",
      " [   6]\n",
      " [8994]\n",
      " [  39]]\n",
      "[[  34]\n",
      " [   4]\n",
      " [   8]\n",
      " [   3]\n",
      " [ 298]\n",
      " [   2]\n",
      " [   6]\n",
      " [1131]\n",
      " [   8]\n",
      " [   9]\n",
      " [  28]\n",
      " [   3]\n",
      " [   2]\n",
      " [   5]\n",
      " [  10]\n",
      " [1188]]\n",
      "[[   3]\n",
      " [  60]\n",
      " [   3]\n",
      " [   9]\n",
      " [ 451]\n",
      " [ 756]\n",
      " [   6]\n",
      " [3398]\n",
      " [   5]\n",
      " [  74]\n",
      " [ 495]\n",
      " [5449]\n",
      " [3761]\n",
      " [   6]\n",
      " [   6]\n",
      " [   2]\n",
      " [  11]]\n",
      "[[  54]\n",
      " [  80]\n",
      " [   3]\n",
      " [4102]\n",
      " [ 310]\n",
      " [1377]\n",
      " [   6]\n",
      " [   3]\n",
      " [  19]\n",
      " [  71]\n",
      " [   3]\n",
      " [  78]\n",
      " [ 709]\n",
      " [ 497]\n",
      " [   5]\n",
      " [   3]\n",
      " [  18]\n",
      " [   3]]\n",
      "[[  80]\n",
      " [ 104]\n",
      " [  45]\n",
      " [  90]\n",
      " [1614]\n",
      " [ 477]\n",
      " [   6]\n",
      " [  70]\n",
      " [  43]\n",
      " [ 222]\n",
      " [ 749]\n",
      " [  61]\n",
      " [   2]\n",
      " [   6]\n",
      " [   5]\n",
      " [ 313]\n",
      " [   7]\n",
      " [   3]\n",
      " [6115]]\n",
      "[[1403]\n",
      " [  35]\n",
      " [  16]\n",
      " [   9]\n",
      " [1135]\n",
      " [4685]\n",
      " [   6]\n",
      " [  44]\n",
      " [3635]\n",
      " [  63]\n",
      " [ 957]\n",
      " [   3]\n",
      " [ 213]\n",
      " [   6]\n",
      " [  30]\n",
      " [  12]\n",
      " [   8]\n",
      " [   2]\n",
      " [3244]\n",
      " [  10]]\n",
      "<s> desperately recorded in every mere function of fellows , for all the <unk> mountains which noted that the actors in \n",
      "<s> import , and <unk> the <unk> <unk> held <unk> and will be taken in the island and normal , the \n",
      "<s> if he said he smiled standing from the solid minutes and had on , though both do not claim it \n",
      "<s> there was succeeded to <unk> them , molding in the conclusions in herself , but for the occupied contribution , \n",
      "<s> cultural children travel . <s> \n",
      "<s> policy drops itself by the chorus of the view whose <unk> lumber itself and <unk> <unk> <unk> , those will \n",
      "<s> it of the united states moves on what they test the more available -- made a man to this other \n",
      "<s> if this illustrated , account of the situation of all side was captured later . <s> \n",
      "<s> you put people to the the bitterness . <s> \n",
      "<s> not it the <unk> of human ship and <unk> , to new <unk> , make a strictly <unk> , blue \n"
     ]
    }
   ],
   "source": [
    "# Same as above, but as a batch\n",
    "max_steps = 20\n",
    "num_samples = 10\n",
    "random_seed = 42\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "with lm.graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(random_seed)\n",
    "    \n",
    "    # Load the trained model\n",
    "#     saver.restore(session, trained_filename)\n",
    "    saver.restore(session, 'rnnlm_trained')\n",
    "\n",
    "    # Make initial state for a batch with batch_size = num_samples\n",
    "    w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    # We'll take one step for each sequence on each iteration \n",
    "    for i in range(max_steps):\n",
    "        h, y = sample_step(lm, session, w[:,-1:], h)\n",
    "        w = np.hstack((w,y))\n",
    "\n",
    "    # Print generated sentences\n",
    "    for row in w:\n",
    "        for i, word_id in enumerate(row):\n",
    "            print(vocab.id_to_word[word_id], end=\" \")\n",
    "            if (i != 0) and (word_id == vocab.START_ID):\n",
    "                break\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) Linguistic Properties (5 points)\n",
    "\n",
    "Now that we've trained our RNNLM, let's test a few properties of the model to see how well it learns linguistic phenomena. We'll do this with a scoring task: given two or more test sentences, our model should score the more plausible (or more correct) sentence with a higher log-probability.\n",
    "\n",
    "We'll define a scoring function to help us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_seq(lm, session, seq, vocab):\n",
    "    \"\"\"Score a sequence of words. Returns total log-probability.\"\"\"\n",
    "    padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq + [\"</s>\"], \n",
    "                                                             wordset=vocab.word_to_id))\n",
    "    w = np.reshape(padded_ids[:-1], [1,-1])\n",
    "    y = np.reshape(padded_ids[1:],  [1,-1])\n",
    "    h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "    feed_dict = {lm.input_w_:w,\n",
    "                 lm.target_y_:y,\n",
    "                 lm.initial_h_:h,\n",
    "                 lm.dropout_keep_prob_: 1.0}\n",
    "    # Return log(P(seq)) = -1*loss\n",
    "    return -1*session.run(lm.loss_, feed_dict)\n",
    "\n",
    "def load_and_score(inputs, sort=False):\n",
    "    \"\"\"Load the trained model and score the given words.\"\"\"\n",
    "    lm = rnnlm.RNNLM(**model_params)\n",
    "    lm.BuildCoreGraph()\n",
    "    \n",
    "    with lm.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=lm.graph) as session:  \n",
    "        # Load the trained model\n",
    "#         saver.restore(session, trained_filename)\n",
    "        saver.restore(session, 'rnnlm_trained')\n",
    "    \n",
    "        if isinstance(inputs[0], str) or isinstance(inputs[0], bytes):\n",
    "            inputs = [inputs]\n",
    "\n",
    "        # Actually run scoring\n",
    "        results = []\n",
    "        for words in inputs:\n",
    "            score = score_seq(lm, session, words, vocab)\n",
    "            results.append((score, words))\n",
    "\n",
    "        # Sort if requested\n",
    "        if sort: results = sorted(results, reverse=True)\n",
    "\n",
    "        # Print results\n",
    "        for score, words in results:\n",
    "            print(\"\\\"{:s}\\\" : {:.02f}\".format(\" \".join(words), score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from rnnlm_trained\n",
      "\"once upon a time\" : -7.68\n",
      "\"the quick brown fox jumps over the lazy dog\" : -7.34\n"
     ]
    }
   ],
   "source": [
    "sents = [\"once upon a time\",\n",
    "         \"the quick brown fox jumps over the lazy dog\"]\n",
    "load_and_score([s.split() for s in sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Number agreement\n",
    "\n",
    "Compare **\"the boy and the girl [are/is]\"**. Which is more plausible according to your model?\n",
    "\n",
    "If your model doesn't order them correctly (*this is OK*), why do you think that might be? (answer in cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from rnnlm_trained\n",
      "\"the boy and the girl are\" : -5.40\n",
      "\"the boy and the girl is\" : -5.28\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "sents = [\"the boy and the girl are\",\n",
    "         \"the boy and the girl is\"]\n",
    "load_and_score([s.split() for s in sents])\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Type/semantic agreement\n",
    "\n",
    "Compare:\n",
    "- **\"peanuts are my favorite kind of [nut/vegetable]\"**\n",
    "- **\"when I'm hungry I really prefer to [eat/drink]\"**\n",
    "\n",
    "Of each pair, which is more plausible according to your model?\n",
    "\n",
    "How would you expect a 3-gram language model to perform at this example? How about a 5-gram model? (answer in cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from rnnlm_trained\n",
      "\"peanuts are my favorite kind of nut\" : -6.91\n",
      "\"peanuts are my favorite kind of vegetable\" : -6.85\n",
      "\"when I'm hungry I really prefer to eat\" : -7.05\n",
      "\"when I'm hungry I really prefer to drink\" : -7.14\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "\n",
    "sents = [\"peanuts are my favorite kind of nut\",\n",
    "         \"peanuts are my favorite kind of vegetable\",\n",
    "         \"when I'm hungry I really prefer to eat\",\n",
    "         \"when I'm hungry I really prefer to drink\"]\n",
    "load_and_score([s.split() for s in sents])\n",
    "\n",
    "\n",
    "#### END(YOUR CODE) ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Adjective ordering (just for fun)\n",
    "\n",
    "Let's repeat the exercise from Week 2:\n",
    "\n",
    "![Adjective Order](adjective_order.jpg)\n",
    "*source: https://twitter.com/MattAndersonBBC/status/772002757222002688?lang=en*\n",
    "\n",
    "We'll consider a toy example (literally), and consider all possible adjective permutations.\n",
    "\n",
    "Note that this is somewhat sensitive to training, and even a good language model might not get it all correct. Why might the NN fail, if the trigram model from Week 2 was able to solve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from rnnlm_trained\n",
      "\"I have lots of green square plastic toys\" : -7.48\n",
      "\"I have lots of green plastic square toys\" : -7.50\n",
      "\"I have lots of square green plastic toys\" : -7.57\n",
      "\"I have lots of square plastic green toys\" : -7.58\n",
      "\"I have lots of plastic green square toys\" : -7.58\n",
      "\"I have lots of plastic square green toys\" : -7.61\n"
     ]
    }
   ],
   "source": [
    "prefix = \"I have lots of\".split()\n",
    "noun = \"toys\"\n",
    "adjectives = [\"square\", \"green\", \"plastic\"]\n",
    "inputs = []\n",
    "for adjs in itertools.permutations(adjectives):\n",
    "    words = prefix + list(adjs) + [noun]\n",
    "    inputs.append(words)\n",
    "    \n",
    "load_and_score(inputs, sort=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
